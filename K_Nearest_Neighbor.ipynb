{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K Nearest Neighbor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoOrN4wEM5t3Fmlh8pZ6z/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lotfullah21/Machine-Learning/blob/main/K_Nearest_Neighbor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nearest Neighbors\n",
        "Nearest neighbor algorithms are among the simplest of all machine learning algorithms.\n",
        "The idea is to memorize the training set and then predict the label of any new instnance on the basis of it's closest neighbor in training set.\n",
        "\n",
        "### Assumptions:\n",
        "data points that are similar, have similar labels, or things that look alike must be alike, or similar inputs have similar outputs.\n"
      ],
      "metadata": {
        "id": "ooruAJWJlEB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbor: \n",
        "in thoery we would always like to predict qualitive response using the Bayes classifier. But for real data we do not the conditiional distribution of Y given X. \n",
        "Given a positive integer K and a test observation x_0, the KNN classifier first identifies the K points in training data that are closest to x_0, represented by Gaussian Distribution. \n",
        "it then estimates the conditional probability for class j as the fraction of points in Gaussian whose response value equal j.\n",
        "Finally, KNN applies Bayes rule and classifies the test observation x_0 to the class with the largest probability.\n",
        "K is usually to be chosen odd."
      ],
      "metadata": {
        "id": "BRd1yFV_lEHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets Make it simpler\n",
        "data points that are similar, have similar labels, and K is the nearest neighbor to a data point.\n",
        "if you are not the K nearest neighbor, then you should be further away than the K nearest neighbor.\n",
        "\n",
        "KNN is only as good as the distant metric, if distant metric reflects simililarity then it is a perfect classifier, By distant metric we mean the distance between two data points"
      ],
      "metadata": {
        "id": "ZGpc4MZqlEK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications\n",
        "#### Classification:\n",
        "we can cast the prediction by majority of the labels for classification.\n",
        "for a test input x assign the most common label amongs it's K most similar training inputs.\n",
        "\n",
        "#### Regression:\n",
        "\n",
        "For regression problems when y is continiuos value like prediction of a house based on the nearest neighbor houses, one can define the prediction to be the average target of the K nearest neighbors.\n"
      ],
      "metadata": {
        "id": "C_uwRN1olEO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm:\n",
        "lets assign: \n",
        "* Sx = {K1,K2,K3,K4,...,KN},\n",
        "* Sx in D, such that |Sx| = K\n",
        "* for all points(x',y') which is in D but not in Sx\n",
        "* dist(x,x') >= max dist(x,x\") \n",
        "\n",
        "x is the data point, x' is the data which we measured distance from x, and x\" is the next data point which we took its distance from x.   \n",
        "\n",
        "Sx doentes the set of K nearest neighbors of data point x.\n",
        "we can define that every point in D but not in Sx is atleast as far away from x as the furthest point in Sx.\n",
        "\n",
        "* h(x) = mode({Y\":(X\",Y\")} IN Sx).\n",
        "\n",
        "the classifier h() is a function that returns the most common label in Sx.\n",
        "\n",
        "#### Note:\n",
        "mode returns the events with the highets occurance.\n"
      ],
      "metadata": {
        "id": "rhj3ScMhlESC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0-xikLrOlEUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ubt3SSIFlEWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8nUnSX4XlEY2"
      }
    }
  ]
}